{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "boston = pd.read_csv('Data_Set/housing.csv')\n",
    "feature_names = ['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'black', 'lstat', 'medv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with high correlation to the housing prices:\n",
      "rm\n",
      "ptratio\n",
      "lstat\n"
     ]
    }
   ],
   "source": [
    "# data cleaning, analysing and removing noise from our model using correlation matrix\n",
    "\n",
    "boston_frame = pd.DataFrame(boston, columns = feature_names)\n",
    "correlation_matirx = boston_frame.corr()\n",
    "\n",
    "# sourceFile = open('Correlation_Matrix.txt', 'w')\n",
    "# print(correlation_matirx, file = sourceFile)\n",
    "# sourceFile.close()\n",
    "\n",
    "correlation_matirx = np.array(correlation_matirx, dtype = 'float32')\n",
    "\n",
    "print(\"Features with high correlation to the housing prices:\")\n",
    "irrelevant_features = []\n",
    "\n",
    "for i in range(len(feature_names) - 1):\n",
    "    if(abs(correlation_matirx[13][i]) > 0.5):\n",
    "        print(feature_names[i])\n",
    "    \n",
    "    else:\n",
    "        irrelevant_features.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset into two parts for training and testing randomly in a ratio of 80:20\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "boston_data = np.array(boston_frame, dtype = 'float32')\n",
    "\n",
    "boston_data = np.delete(boston_data, (irrelevant_features), 1)\n",
    "features = boston_data[:, :-1]\n",
    "prices = boston_data[:, -1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, prices, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the data through feature scaling\n",
    "\n",
    "def normalize(x):\n",
    "    return (x - x.mean(axis = 0)) / x.std(axis = 0)\n",
    "\n",
    "x_train = normalize(x_train)\n",
    "y_train = normalize(y_train)\n",
    "x_test = normalize(x_test)\n",
    "y_test = normalize(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# number of training examples\n",
    "m = len(y_train)\n",
    "\n",
    "def error(m, w0, w1, w2, w3, x, y):\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        cost += ((w0 + w1*x[i][0] + w2*x[i][1] + w3*x[i][2] - y[i]) ** 2)\n",
    "\n",
    "    return cost / (0.5 * m)\n",
    "\n",
    "def gradient_descent(alpha, w0, w1, w2, w3, x, y):\n",
    "    temp0, temp1, temp2, temp3 = 0, 0, 0, 0\n",
    "    for i in range(m):\n",
    "        temp = (w0 + w1*x[i][0] + w2*x[i][1] + w3*x[i][2] - y[i])\n",
    "        temp0 += temp\n",
    "        temp1 += (temp * x[i][0])\n",
    "        temp2 += (temp * x[i][1])\n",
    "        temp3 += (temp * x[i][2])\n",
    "    \n",
    "    w0 -= (alpha * temp0 / m)\n",
    "    w1 -= (alpha * temp1 / m)\n",
    "    w2 -= (alpha * temp2 / m)\n",
    "    w3 -= (alpha * temp3 / m)\n",
    "\n",
    "    return w0, w1, w2, w3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5728862894636689 0.20699271117883422 0.930205511570624 0.36207233288176477\n",
      "7.078332257541675\n",
      "6.457036629899338\n",
      "5.9000475060750475\n",
      "5.40048393352039\n",
      "4.952212602442443\n",
      "4.549766025957399\n",
      "4.188269705241553\n",
      "3.863377291333574\n",
      "3.5712128640512066\n",
      "3.308319545310475\n",
      "3.0716137502963603\n",
      "2.858344456606075\n",
      "2.666056939713794\n",
      "2.4925604838202537\n",
      "2.3358996311789024\n",
      "2.194328581069199\n",
      "2.0662883923722517\n",
      "1.9503866817778286\n",
      "1.8453795435331724\n",
      "1.750155446795535\n",
      "1.6637208934818501\n",
      "1.5851876433870535\n",
      "1.513761334591774\n",
      "1.4487313460904305\n",
      "1.3894617663996456\n",
      "1.33538334688341\n",
      "1.2859863318599065\n",
      "1.2408140694163583\n",
      "1.1994573174147902\n",
      "1.1615491685663555\n",
      "1.1267605268131384\n",
      "1.0947960746977663\n",
      "1.065390678024027\n",
      "1.0383061800061253\n",
      "1.0133285423503495\n",
      "0.9902652953822918\n",
      "0.9689432634885818\n",
      "0.9492065358412204\n",
      "0.930914655664923\n",
      "0.9139410042385427\n",
      "0.898171358430145\n",
      "0.8835026028872446\n",
      "0.8698415800706281\n",
      "0.8571040631600417\n",
      "0.8452138384978464\n",
      "0.8341018856948051\n",
      "0.8237056448201661\n",
      "0.8139683612537097\n",
      "0.8048384998062931\n",
      "0.796269220631315\n",
      "0.7882179102650464\n",
      "0.7806457618599486\n",
      "0.7735173993215494\n",
      "0.7668005406353029\n",
      "0.7604656961823955\n",
      "0.7544858983001779\n",
      "0.7488364587493163\n",
      "0.7434947511120825\n",
      "0.7384400154685654\n",
      "0.7336531829849945\n",
      "0.7291167183042284\n",
      "0.724814477856423\n",
      "0.7207315824110486\n",
      "0.7168543023723462\n",
      "0.71316995448153\n",
      "0.7096668087327944\n",
      "0.7063340044381546\n",
      "0.7031614744903193\n",
      "0.7001398769744805\n",
      "0.6972605333706394\n",
      "0.6945153726688597\n",
      "0.6918968807919843\n",
      "0.6893980547846087\n",
      "0.6870123612844178\n",
      "0.6847336988431783\n",
      "0.6825563637102381\n",
      "0.6804750187321447\n",
      "0.6784846650582824\n",
      "0.6765806163748719\n",
      "0.6747584754186084\n",
      "0.6730141125470575\n",
      "0.6713436461660166\n",
      "0.6697434248346524\n",
      "0.6682100108876597\n",
      "0.6667401654301218\n",
      "0.665330834575505\n",
      "0.6639791368103297\n",
      "0.662682351380866\n",
      "0.6614379076077042\n",
      "0.660243375043477\n",
      "0.659096454397449\n",
      "0.6579949691582226\n",
      "0.6569368578526007\n",
      "0.6559201668846855\n",
      "0.654943043904725\n",
      "0.6540037316621261\n",
      "0.65310056230138\n",
      "0.6522319520636068\n",
      "0.6513963963599328\n",
      "0.6505924651861\n",
      "Accuracy: 0.7965494063001551\n"
     ]
    }
   ],
   "source": [
    "w0, w1, w2, w3 = random.random(), random.random(), random.random(), random.random()\n",
    "# print(error(w0, w1, w2, w3, x_train, y_train))\n",
    "print(w0, w1, w2, w3)\n",
    "\n",
    "for _ in range(100):\n",
    "    print(error(m, w0, w1, w2, w3, x_train, y_train))\n",
    "    w0, w1, w2, w3 = gradient_descent(0.03, w0, w1, w2, w3, x_train, y_train)\n",
    "\n",
    "m = len(y_test)\n",
    "print(\"Accuracy:\", error(m, w0, w1, w2, w3, x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c2f2c10e8b0c6806d9d04e8d3a17761b4c554a55a9bbe8b591472136559041bf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
